{
  "updatedAt": "2025-11-08T00:46:55.000Z",
  "createdAt": "2025-10-27T22:27:12.292Z",
  "id": "r12Z7cpPLhCgVVRI",
  "name": "@FORMAT_TEXT_GENERATION",
  "description": null,
  "active": false,
  "isArchived": false,
  "nodes": [
    {
      "parameters": {
        "model": {
          "__rl": true,
          "value": "o3-mini-2025-01-31",
          "mode": "list",
          "cachedResultName": "o3-mini-2025-01-31"
        },
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "typeVersion": 1.2,
      "position": [
        448,
        448
      ],
      "id": "41ebdc21-ee12-40ff-b940-dc02bb6a7ba2",
      "name": "OpenAI Chat Model",
      "credentials": {
        "openAiApi": {
          "id": "VYXpJ372t4VzzgvC",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "jsonSchemaExample": "{\n  \"imageStyle\": \"normalized-string\",\n  \"scenes\": [\n    {\n      \"index\": 0,\n      \"introduction\": true,      \n      \"paragraphs\": [\n        { \"id\": 0, \"text\": \"...\", \"voiceId\":\"...\" },\n        { \"id\": 1, \"text\": \"...\", \"voiceId\":\"...\" }\n      ]\n    }\n  ]\n}"
      },
      "type": "@n8n/n8n-nodes-langchain.outputParserStructured",
      "typeVersion": 1.3,
      "position": [
        864,
        464
      ],
      "id": "ca829eb4-c7c2-47a3-a996-948e101aaed6",
      "name": "Structured Output Parser"
    },
    {
      "parameters": {
        "options": {
          "prompt": "Instructions:\n--------------\n{instructions}\n--------------\nCompletion:\n--------------\n{completion}\n--------------\n\nAbove, the Completion did not satisfy the constraints given in the Instructions.\nError:\n--------------\n{error}\n--------------\n\nPlease try again. Please only respond with an answer that satisfies the constraints laid out in the Instructions:"
        }
      },
      "type": "@n8n/n8n-nodes-langchain.outputParserAutofixing",
      "typeVersion": 1,
      "position": [
        816,
        240
      ],
      "id": "5d0d46e0-4a47-4b61-a877-3f552e081386",
      "name": "Auto-fixing Output Parser"
    },
    {
      "parameters": {
        "inputSource": "jsonExample",
        "jsonExample": "\n  {\n    \"rawText\": \"Passado simples, participio passado e passado continuo\",        \n    \"imageStyle\": \"Anime estilo 3D\",\n    \"scenes\": 4,\n    \"targetWordsPerParagraph\": 45\n  }\n"
      },
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1.1,
      "position": [
        368,
        0
      ],
      "id": "9b5745a7-0a33-4686-930b-1ac732f2a2d9",
      "name": "START"
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "=text: {{ $('START').item.json.rawText }}\nimageStyle:{{ $('START').item.json.imageStyle }}\nscenes:{{ $('START').item.json.scenes }}\ntargetWordsPerParagraph:{{ $('START').item.json.targetWordsPerParagraph }}\navailableVoices: {{ JSON.stringify($('@GLOBAL_SHADOWING').item.json.AVAILABLE_VOICES) }}",
        "hasOutputParser": true,
        "options": {
          "systemMessage": "=# System Prompt — Scene Segmentation & Visual Description Agent (Updated)\n\nYou are an agent that **receives a long text** and an **image style**, and returns a **structured JSON** that:\n1. **Splits the text into coherent scenes** (narrative or thematic units).\n2. For **each scene**, produces a **detailed image description** (following the requested style: *realistic, 3D, anime, cartoon, etc.*).\n3. **Groups the original paragraphs** that belong to each scene, **preserving order** and recording their original indices.\n4. Marks whether a scene is an **introduction** via the boolean field `introduction`.\n\n---\n\n## Inputs\n- `text` (string): the full text to process.\n- `imageStyle` (string): one of `{realistic | 3d | anime | cartoon | illustration | painting | lowpoly | pixelart | manga | photographic}` (accept variations in case/accents; normalize internally).\n- `maxSceneLength` (optional, integer): suggested approximate maximum length per scene (in paragraphs). Treat it as a **hint**, not a rigid rule.\n- `sceneCount` (optional, integer): **target number of scenes**. When provided, **produce exactly this number of scenes** while keeping semantic coherence.\n- `availableVoices` (array of objects): e.g.  \n  `[{\"voiceId\":\"ChO6kqkVouUn0s7HMunx\",\"description\":\"middle-aged male\"},{\"voiceId\":\"4YYIPFl9wE5c4L2eu2Gb\",\"description\":\"deep strong male narrator voice\"},{\"voiceId\":\"SAz9YHcvj6GT2YYXdXww\",\"description\":\"middle-aged female\"},{\"voiceId\":\"ZF6FPAbjXT4488VcRRnw\",\"description\":\"teenage female\"}]`\n\n---\n\n## Scene Segmentation Rules\n1. **Intro scene (mandatory, synthesized)**:\n   - **Scene 0** is **always** an introduction with **exactly one paragraph** and `introduction: true`.\n   - **This paragraph is GENERATED by the agent** (it does **not** come from the original text paragraphs).\n   - Content requirements (keep concise, 1–3 sentences):\n     - **State the theme of the shadowing text** (extract the most fitting theme from the full input; e.g., “Theme: directions in a historical town.”).\n     - Explain the **3-pass method** for shadowing: **(1)** first pass, the user **listens** attentively; **(2)** second pass, the user **repeats after**; **(3)** third pass, the user **speaks together**.\n     - Add a brief **encouragement** line (e.g., “breathe, articulate clearly, and keep pace”).\n     - Add at the end: Study materials are linked in the video description\n   - Language of the intro must **match the input language**.\n   - The image description for scene 0 should be a **neutral title-card visual** consistent with `imageStyle` (no brands/logos).\n   - **Paragraph mapping for scene 0**: use a single synthetic paragraph object with `\"id\": -1`, the generated `\"text\"`, and an assigned `\"voiceId\"` (prefer a narrator voice if available).\n2. **Semantic criterion (remaining scenes)**: create a new scene when there is a clear change of **location**, **time**, **character focus**, **event**, or **subtopic**.\n3. **Internal cohesion**: each scene must form a cohesive unit; avoid one-paragraph scenes unless there is a clear shift (note: scene 0 is intentionally one paragraph).\n4. **Full coverage of original text**: every **original** paragraph from the input must belong to **exactly one** scene **from index ≥ 1**. **Do not omit** paragraphs.\n5. **Preserve order**: scenes and paragraphs within each scene must follow the original order.\n6. **Granularity**: prefer **fewer, well-defined scenes** over excessive fragmentation. Use `maxSceneLength` only as guidance.\n7. **Targeted distribution (when `sceneCount` is provided)**: distribute **original** paragraphs across exactly `sceneCount` scenes so that:\n   - All scenes are **non-empty** (scene 0 is the synthetic intro; the remaining `sceneCount-1` scenes must cover all original paragraphs).\n   - Scene sizes are **balanced** (difference of at most one paragraph where possible).\n   - **Semantic boundaries** are respected first; when semantics and exact balancing conflict, **merge or slightly shift** adjacent boundaries to achieve both coherence and the requested count.\n   - **Keep scene 0 fixed** as the intro with one synthetic paragraph.\n\n---\n\n## Image Description (per scene)\nProduce a **single** description per scene in clear, objective language, suitable for image generation. Follow:\n- **Style**: explicitly integrate `imageStyle` (e.g., “anime style”, “physically-based 3D render”, “realistic 50mm photography”, etc.).\n- **Visual content**: include **primary subjects**, **action/pose**, **setting/environment**, **lighting**, **color palette**, **composition/framing**, **mood/atmosphere**, and **level of detail**.\n- **Caption-friendly composition (required)**: the image must be **simple and uncluttered** to support an overlaid caption. **Avoid** dense textures, fine or busy line-work, heavy patterns, excessive micro-detail, or visual noise. Prefer **clean shapes**, **ample negative space**, **stable backgrounds** with **moderate contrast**, and a clear focal area that **does not compete** with the caption.\n- **ABSOLUTE RESTRICTION**: **no text in the image, ever**. The **description must explicitly include** a clear instruction such as **“no text in the image”**.\n- **Restrictions**: do not inject proper names or facts **not present** in the text; **no** brands/logos; **do not** copy non-visual sentences from the text.\n- **Intro scene hint**: prefer a clean **title card** or abstract motif related to the theme and the 3-pass method (generic icons like headphones or sound waves are acceptable and neutral).\n\n---\n\n## Paragraph Mapping\n- Split `text` into paragraphs using **blank lines** as separators (or `\\n\\n` where applicable).\n- **Scene 0 (synthetic)**:\n  - `paragraphs` contains **one** object:  \n    `{ \"id\": -1, \"text\": \"<generated intro per rules>\", \"voiceId\": \"<assigned from availableVoices (prefer narrator)>\" }`.\n- **Scenes 1..k (original text only)**:\n  - For each scene, return an array `paragraphs` of objects:\n    - `id` (integer): the **global** paragraph index in the original text, starting at 0.\n    - `text` (string): the **exact** original paragraph content (no rewriting).\n    - `voiceId` (string): **must** be assigned from one of the entries in `availableVoices` according to the **Voice Assignment Rules** below.\n- **Do not alter** punctuation, diacritics, or capitalization of original paragraphs.\n- target Words Per Paragraph: targetWordsPerParagraph\n\n---\n\n## Voice Assignment Rules\nSelect `voiceId` **per paragraph** using `availableVoices`. Use semantics to guide assignment:\n- **Narration / neutral exposition**: prefer `\"deep strong male narrator voice\"` (`voiceId: \"4YYIPFl9wE5c4L2eu2Gb\"`) or another clearly “narrator” voice if provided.\n- **Adult male speaker**: prefer `\"middle-aged male\"` (`\"ChO6kqkVouUn0s7HMunx\"`).\n- **Adult female speaker**: prefer `\"middle-aged female\"` (`\"SAz9YHcvj6GT2YYXdXww\"`).\n- **Teen/younger speaker**: prefer `\"teenage female\"` (`\"ZF6FPAbjXT4488VcRRnw\"`).\n- In **dialogues**, alternate voices consistent with speaker identity; keep a **single voice per paragraph**.\n- If ambiguity remains, default to the **narrator** voice for continuity.\n\n---\n\n## Required Validations\n- **Coverage of original text**: across scenes with `index ≥ 1`, all `id` indices must be **contiguous** and cover **0..N-1** with no gaps or duplicates.\n- **Exclusion of synthetic intro from coverage**: the paragraph with `id: -1` in scene 0 is **synthetic** and **must not** appear in the 0..N-1 range.\n- Each paragraph (original) must belong to **only one** scene (index ≥ 1).\n- `scenes` must not be empty.\n- **Scene 0**:\n  - `index === 0`\n  - `introduction === true`\n  - `paragraphs.length === 1` with `id === -1` and a valid `voiceId`.\n- **Scenes 1..k**:\n  - `introduction === false`\n  - each contains **at least one** original paragraph and each paragraph has a valid `voiceId` from `availableVoices`.\n- If `sceneCount` is provided, the length of `scenes` **must equal** `sceneCount`.\n\n---\n\n## Output (format and JSON only)\nReturn **only** a valid JSON following **exactly** this structure:\n\n```json\n{\n  \"imageStyle\": \"normalized-string\",\n  \"scenes\": [\n    {\n      \"index\": 0,\n      \"introduction\": true,\n      \"paragraphs\": [\n        { \"id\": -1, \"text\": \"Theme: <auto-extracted theme>. Shadowing in 3 passes — first, listen attentively; second, repeat after the reading; third, speak together. Keep a steady pace, breathe, and articulate clearly.\", \"voiceId\": \"<from availableVoices (prefer narrator)>\" }\n      ]\n    },\n    {\n      \"index\": 1,\n      \"introduction\": false,\n      \"paragraphs\": [\n        { \"id\": 0, \"text\": \"...\", \"voiceId\": \"<from availableVoices>\" },\n        { \"id\": 1, \"text\": \"...\", \"voiceId\": \"<from availableVoices>\" }\n      ]\n    }\n  ]\n}\n"
        }
      },
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 2.2,
      "position": [
        752,
        0
      ],
      "id": "1c8a9fdd-a8e4-4e8f-84b9-1e6a821cd2ee",
      "name": "AI Scenes Generator"
    },
    {
      "parameters": {
        "jsCode": "// Retorna um array de itens n8n, um por cena: [{ json: scene }, ...]\nconst out = [];\n\nfor (const item of items) {\n  const root = Array.isArray(item.json) ? item.json[0] : item.json;\n  const scenes = root?.output?.scenes ?? [];\n  for (const scene of scenes) out.push({ json: scene });\n}\n\nreturn out;\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1072,
        0
      ],
      "id": "b79f82ea-e9f4-4aa3-bf67-0a8e9d5919e7",
      "name": "Splice scenes"
    },
    {
      "parameters": {
        "workflowId": {
          "__rl": true,
          "value": "VdmO0DZ3i6iBFRiU",
          "mode": "list",
          "cachedResultUrl": "/workflow/VdmO0DZ3i6iBFRiU",
          "cachedResultName": "@GLOBAL_SHADOWING"
        },
        "workflowInputs": {
          "mappingMode": "defineBelow",
          "value": {}
        },
        "options": {}
      },
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1.3,
      "position": [
        544,
        0
      ],
      "id": "cfcfb6f7-ddd7-4baa-8473-746c75975866",
      "name": "@GLOBAL_SHADOWING"
    }
  ],
  "connections": {
    "OpenAI Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "AI Scenes Generator",
            "type": "ai_languageModel",
            "index": 0
          },
          {
            "node": "Auto-fixing Output Parser",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Structured Output Parser": {
      "ai_outputParser": [
        [
          {
            "node": "Auto-fixing Output Parser",
            "type": "ai_outputParser",
            "index": 0
          }
        ]
      ]
    },
    "Auto-fixing Output Parser": {
      "ai_outputParser": [
        [
          {
            "node": "AI Scenes Generator",
            "type": "ai_outputParser",
            "index": 0
          }
        ]
      ]
    },
    "START": {
      "main": [
        [
          {
            "node": "@GLOBAL_SHADOWING",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "AI Scenes Generator": {
      "main": [
        [
          {
            "node": "Splice scenes",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Splice scenes": {
      "main": [
        []
      ]
    },
    "@GLOBAL_SHADOWING": {
      "main": [
        [
          {
            "node": "AI Scenes Generator",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "settings": {
    "executionOrder": "v1"
  },
  "staticData": null,
  "meta": {
    "templateCredsSetupCompleted": true
  },
  "pinData": {
    "START": [
      {
        "json": {
          "rawText": "Passado simples, participio passado e passado continuo",
          "imageStyle": "Anime estilo 3D",
          "scenes": 4,
          "targetWordsPerParagraph": 45
        }
      }
    ]
  },
  "versionId": "1787da30-6f38-485c-8087-ba9c6f7ad2c2",
  "versionCounter": 2,
  "triggerCount": 0,
  "shared": [
    {
      "updatedAt": "2025-10-27T22:27:12.323Z",
      "createdAt": "2025-10-27T22:27:12.323Z",
      "role": "workflow:owner",
      "workflowId": "r12Z7cpPLhCgVVRI",
      "projectId": "TH7SrF7Rx9ho2z9i",
      "project": {
        "updatedAt": "2025-08-20T20:53:20.326Z",
        "createdAt": "2025-08-20T20:52:17.088Z",
        "id": "TH7SrF7Rx9ho2z9i",
        "name": "Hugo Dutra <hugo.dutra@hotmail.com>",
        "type": "personal",
        "icon": null,
        "description": null,
        "projectRelations": [
          {
            "updatedAt": "2025-08-20T20:52:17.089Z",
            "createdAt": "2025-08-20T20:52:17.089Z",
            "userId": "96b62bee-ebb1-4fe5-a51b-c116ef7064eb",
            "projectId": "TH7SrF7Rx9ho2z9i",
            "user": {
              "updatedAt": "2026-01-15T11:12:49.000Z",
              "createdAt": "2025-08-20T20:52:15.066Z",
              "id": "96b62bee-ebb1-4fe5-a51b-c116ef7064eb",
              "email": "hugo.dutra@hotmail.com",
              "firstName": "Hugo",
              "lastName": "Dutra",
              "personalizationAnswers": {
                "version": "v4",
                "personalization_survey_submitted_at": "2025-08-20T20:56:08.200Z",
                "personalization_survey_n8n_version": "1.107.4",
                "automationGoalDevops": [
                  "other"
                ],
                "automationGoalDevopsOther": "Video As A Service",
                "companySize": "<20",
                "companyType": "saas",
                "role": "engineering",
                "reportedSource": "youtube"
              },
              "settings": {
                "userActivated": true,
                "firstSuccessfulWorkflowId": "DOfnd0M834V8kbv2",
                "userActivatedAt": 1755725592234,
                "npsSurvey": {
                  "responded": true,
                  "lastShownAt": 1757188846991
                },
                "easyAIWorkflowOnboarded": true,
                "dismissedCallouts": {
                  "preBuiltAgentsCalloutHttpRequest": true,
                  "preBuiltAgentsModalCallout": true
                }
              },
              "disabled": false,
              "mfaEnabled": false,
              "lastActiveAt": "2026-01-15",
              "isPending": false
            }
          }
        ]
      }
    }
  ],
  "tags": []
}